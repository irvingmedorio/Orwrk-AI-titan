# Backend API Configuration
ENVIRONMENT=development
SECRET_KEY=your_super_secret_key_here

# LLM Provider Configuration
# Set this to 'local' or 'external'
LLM_PROVIDER=local

# Llama.cpp Server Configuration (for local LLM)
# Host is the service name in docker-compose
LLAMA_CPP_HOST=http://llama-cpp:8080
# This command starts the server in the llama-cpp container.
# The user MUST change the model file to the one they downloaded from https://leap.liquid.ai/models
# Example using the model specified by the user:
LLAMA_CPP_COMMAND="--model /models/LFM2-1.2B.gguf --host 0.0.0.0 --port 8080"

# External LLM API Keys (if LLM_PROVIDER=external)
GEMINI_API_KEY=your_gemini_api_key
OPENAI_API_KEY=your_openai_api_key
ANTHROPIC_API_KEY=your_anthropic_api_key

# Redis Configuration (for Celery)
# Host is the service name in docker-compose
CELERY_BROKER_URL=redis://redis:6379/0
CELERY_RESULT_BACKEND=redis://redis:6379/0

# ChromaDB Configuration
# Host is the service name in docker-compose
CHROMA_DB_HOST=chromadb
CHROMA_DB_PORT=8000

# Cloud Storage OAuth2 Credentials
# These will be used by the CloudAgent
GOOGLE_CLIENT_ID=your_google_client_id.apps.googleusercontent.com
GOOGLE_CLIENT_SECRET=your_google_client_secret
MICROSOFT_CLIENT_ID=your_microsoft_client_id
MICROSOFT_CLIENT_SECRET=your_microsoft_client_secret
