version: "3.9"

services:
  backend-api:
    build: ./backend
    env_file: backend/.env
    ports:
      - "8000:8000"
    depends_on:
      llama.cpp:
        condition: service_healthy
      redis:
        condition: service_started
    volumes:
      - workspaces:/workspaces
      - /var/run/docker.sock:/var/run/docker.sock
      - ./logs:/logs

  frontend:
    build:
      context: .
      dockerfile: frontend/Dockerfile
    ports:
      - "3000:80"
    depends_on:
      - backend-api

  celery-worker:
    build: ./backend
    command: celery -A app.celery_app.celery_app worker --loglevel=info
    env_file: backend/.env
    depends_on:
      - redis

  redis:
    image: redis:7
    ports:
      - "6379:6379"

  chromadb:
    image: chromadb/chroma
    ports:
      - "8001:8000"

  llama.cpp:
    image: ghcr.io/ggerganov/llama.cpp:latest
    command: ["--model", "/models/lfm2-vl-1.6b-q4_0.gguf", "--host", "0.0.0.0", "--port", "8080", "--api", "--n-gpu-layers", "999"]
    volumes:
      - ./models:/models
    ports:
      - "8080:8080"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 10s
      timeout: 5s
      retries: 5

volumes:
  workspaces:
